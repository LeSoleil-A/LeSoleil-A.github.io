---
layout: post
title:  "笔记《Video Background Music Generation with Controllable Music Transformer》"
date:   2021-10-29 9:30:00 +0800
categories: 音乐论文笔记
typora-root-url: ..\assets\img\1029

---



这篇文章2021年ACMM会议的最佳论文，文章的主要工作是以视频信息为依据的背景音乐生成。

这篇论文有以下几个值得注意的点：

1. 对于视频的处理，如何从视频中提取有效信息；
2. 如何建立视频与音乐之间的关系，选择哪些指标来建立，以及视频如何引导音乐生成；
3. 文章的逻辑比较清晰，可以学习它的组织结构。

<br>

### 摘要

<br>

【背景与目标】

本文所做的主要工作是视频的背景音乐生成。在前人的工作中，虽然能够做到有效生成，但是无法专门对某一个给定的视频生成富有旋律的音乐，而且尚没有工作考虑视频与音乐之间的节奏一致性。

【方法】

为了生成视听一致的背景音乐，本文首先建立了视频与背景音乐之间的节奏关系。值得注意的是，本文将视频中的**时间、动作、速度和动作凸显点**与音乐中的**节奏、同时音密度、同时音力度**等一对一联系起来。

在模型上，本文提出了**CMT**模型（Controllable Music Transformer），以控制上述韵律特征，另外用户还可以指定音乐种类和所使用的乐器。

【结果】

经过主观和客观两方面的评估，结果表明了生成的背景音乐与输入的视频达成了令人满意的一致性，同时保证了生成音乐的质量。

<br>

### 介绍

<br>

首先文章论述了**AI视听融合生成的必要性**。主要指出了要生成与视频相符的音乐，需要很高的技术门槛、大量时间投入以及版权冲突。然而现在少有音乐生成工作考虑到音乐-视频的韵律一致性。

接下来简要**介绍了本文的工作**：视频背景音乐生成。不同于数据驱动的传统方法，本文提出了一个基于 *transformer* 的方法，可以摆脱对于有标记训练数据的依赖。下图是对于本文工作的一个简要介绍。

![overview](/assets/img/1029/overview.PNG)

文章接下来指出了**在视频与音乐之间建立的三个韵律关系**：

1. **动作速度（motion speed）与同时音密度（ simu-note density ）**：动作速度为使用光流法计算出的一小截视频片段中画面的变化幅度，同时音密度为每小节的同时音数量。同时音即同时开始发生的音符组。
2. **局部动作凸显点（motion saliency）与同时音强度（ simu-note strength ）**：局部动作凸显点例如一个场景的边界等，同时音强度为一个同时音中音符的数量。这两个要对应起来，以使得观众获得视听上的节奏统一感。
3. **节奏-时间点编码（beat-timing encoding）**：视频的开场、收场与背景音乐需要同步，时长也要统一，所以视频的时间信息也会用于指导音乐生成。

<br>

下面一段主要讲述了**实验使用的模型**：

在**数据处理**上，实验中的音乐表示基于 *compound words* （具体是什么不是很清楚，要阅读一下相关的文献）。将一组邻居音符根据种类建立二维表示，二维是指 *note-related* 和 *rhythm-related* 两个层面，可以理解为音高和节奏两个维度。此外，还加入了由用户定义的音乐种类和乐器作为其他指标。

在**模型选择**上，选择了 *linear transformer* 模型作为音乐生成pipeline的backbone。选择这一模型的原因主要是考虑到了它的**轻量性**和**注意力计算的线性复杂度**。这篇文章的音乐生成分为了两个阶段：**train stage和inference stage**，按照个人的理解，*train stage* 主要是在处理音乐部分（学习相关音乐属性的含义），而 *inference stage* 是在提取视频的视觉特征，用视频信息替换前一个阶段的音乐属性信息（例如，用视频速度替换同时音密度这个指标，用视频凸显点替换同时音力度），从而生成基于视频特点的音乐。在 *train stage* ，数据集使用了 *LPD* （Lakh Pianoroll Dataset）。

<br>

**总体来说，本篇文章做出了以下三个主要贡献**：

1. 针对视频背景音乐生成任务，提出了 *CMT* 模型，虽然利用了视频与音乐之间的关系，但是不需要打标好的视频-音乐对；
2. 引入了对于音乐的表示，包括同时音的密度和力度。
3. 本文的方法，成功将音乐与视频的节奏和情感联系了起来，同时保证了生成音乐的质量。

<br>

### 相关工作

<br>

**【音乐表示】**

**大多数工作：**MIDI-like event sequence；

**REMI**：将输入表示为矩阵结构。例如为小节、节奏点、和弦、速度等参量提供明确的标记。其优点在于：有利于保持局部速度变化的灵活性，提供了可供掌握音乐韵律与和谐度的基础。

**Compound words（CP）：**通过将邻居音符分组，将 **REMI** 符号转变为一个 *compound words* 序列。其优点在于：大幅减少了音符序列的长度。

**本文：**本文的表示以 *CP* 为基础。将音乐标记分为节奏相关与音高相关。本文加入了音乐种类、乐器种类以及同时音密度与力度。

<br>

**【音乐生成模型】**

**自动编码器**：近年的一些工作使用自动编码器学习符号音乐的隐藏空间，生成新的音乐片段。

**转为图像**：有一些工作将钢琴条窗转为2-D图片，然后建立CNN模型。

**NLP**：基于音乐与语言都可以表示为序列，*transformer模型及其变种* 常作为音乐生成模型的 backbone。

**直接生成音频**：不生成音乐的符号表示，而是直接生成波形音频，或者通过编曲与合成间接生成。

**本文**：基于 *linear transformer* 的模型，采用了一种线性注意力机制以减少时间复杂度。

<br>

**【通过无声视频生成音乐】**

**其他工作**：主要依据视频中的人物在弹奏什么乐器进行生成。很多生成结果，例如乐器种类甚至是节奏等，是由视频中人的动作直接提取生成的，所以生成的音乐实际上有很强的确定性。

**本文工作**：相较来说，本文的方法更适合通常的视频，可以生成不确定的结果。而且目前没有针对视频背景音乐生成的数据集。在一些现有的视听数据集中，常常包含人声等噪音，或者不涉及到任何音乐。由于这种有标记数据的缺失，传统的监督学习方法不可行，本文提出了基于视频动作凸显点、视觉节奏、全局时间信息以及用户定义的音乐种类和乐器等指标生成背景音乐的方法。

<br>

### 如何建立视频与音乐之间的韵律关系

<br>

![relationship](/assets/img/1029/relationship.PNG)

**【视频时间与音乐节奏】**

定义了两个公式。*f~beat~（t）* 表示到视频的第 t 帧是音乐的第几拍；*f~frame~(i)* 表示到音乐的第 i 拍是视频的第几帧。

![beat](/assets/img/1029/beat.PNG)

![frame](/assets/img/1029/frame.PNG)

*Tempo* 为背景音乐的速度，*FPS* 为每秒几帧。十六分音符为最小单位。

<br>

**【视频速度与同时音密度】**

首先是**视频速度的定义**：

将整个视频分为 M 个片段，M 定义为**总的节拍数除以每一组的节拍数后取上界**。本文将每一组的节拍数设置为4。然后根据光流法计算每一个视频片段的动作速度。

接下来是对于**光流法的简单介绍**。首先，*optical flow field* 衡量了两个连续帧中单个像素点的位移大小。然后定义了 *optical flow magnitude* ，即绝对光流的平均值，以衡量第 t 帧的动作大小。

然后便可以定义第 m 个视频片段的**视频速度**，即该片段中每一帧的 *optical flow magnitude* 的平均值。这个公式不是很理解，但是大致可以认为是这一个片段所有帧中像素点的变化幅度。

![speed](/assets/img/1029/speed.PNG)

然后是**同时音密度的定义**：

首先定义**同时音**。同时音即一组开始时间相同的音符：*simu-note~i,j,k~* = { n~1~, n~2~, ..., n~N~ }，其中 i 表示第 i 个小节，j 表示改小节中第 j 个十六分音符，k 表示乐器， n 表示一个单独的音符。这种表示更强调整体性和韵律特征。

此外，一个小节可以被表示为一组同时音的非空集合。

接下来便可以定义**同时音密度**。一个小节的同时音密度是这一个小节中，存在同时音的十六分音数目。

**将视频速度与同时音密度统一的方法**如下：由于一小节有四拍，所以总共会产生16个同时音密度值，于是本文将视频速度也划分为16个档次。比如，有5%的小节同时音密度值为16，那么便将速度占前5%的视频的同时音密度值标记为16。由于一个视频片段也包含4拍，与一个小节一样，所以，在 *inference stage* 会将同时音密度用分类好的视频速度来替代。

<br>

**【动作凸显点与同时音力度】**

首先定义了**动作凸显点**，其被定义为两个连续帧之间所有方向上光流的平均正变化。

接下来获得**visual beats**。通过选择既有**局部最大动作凸显点**同时又有**近乎不变的tempo**的帧序列，从而得到一系列的 *visual beats*。每一个 *visual beat* 都被表示为一个二元组 *（t，s）*，其中 *t* 是帧序号，*s* 是这一帧的动作凸显程度。当画面中出现了剧烈的视觉变化时，*s* 会得到一个较大的值。

接下来定义**同时音力度**，可以表示为**一组同时音中音符的数量**。同时音力度代表了和弦的丰富度，给听众一种韵律感和听觉冲击感。

<br>

### Controllable Music Transformer

<br>

模型示意如图所示。*train stage* 只使用从MIDI文件中提取的韵律信息，*inference stage* 将韵律特征替换为从视频中提取的视觉特征。

![CMT](/assets/img/1029/CMT.PNG)

<br>

**【音乐表示】**

数据创新：将相关属性整合为一个单独的标记，以缩短序列长度。

本文考虑了七种属性：**类型（type）、每小节的节拍数（beat/bar）、密度(density)、力度(strength)、乐器(instrument)、音高(pitch)、持续时间(duration)**。并且将其中属性分为两组，即**节奏相关**（节拍、密度、力度）和**韵律相关**（音高、持续时间和乐器类型）。通过**类型**这一变量识别是哪一组属性。为了保证可行性，本文将节奏相关标记（rhythm-related token）中的韵律属性都标记为None，反之亦然。每一个节奏标记都包含力度这一个变量，来表示应该生成的音符数量；另外，每一个小节中的密度值都单调递减，标识小节中剩余的同时音组数量。每一个标记的嵌入向量由下式计算而得：

![embedding](/assets/img/1029/embedding.PNG)

其中，w 是属性 k 在 i 时刻的输入值，K 是属性数目（此处为7），p是嵌入后的结果，W 是一个线性层，将连接后的嵌入向量映射到R。这里R的维度d被设定为512，x是嵌入后的标记。

另外，**音乐种类与乐器种类**这两个指标作为每个音乐的起始标记，并且为它应用一个独立的嵌入层。嵌入层的大小和普通的标记一样。

<br>

**【Control】**

完成训练后，CMT应该已经能够理解力度、密度等概念的含义了。接下来要考虑的问题，在 *inference stage* ，**如何将这两个指标替换为视觉信息**。

**密度替换**：

将**每小节标记的密度属性**替换为**视频的密度值**。CMT会在该小节中自动生成相应的同时音组数。

**力度替换**：

将**此节拍标记的力度属性**替换为视频的**visual beat及其力度**。CMT会在该节拍处生成相应数量的音符符号。

**超参C — 可控程度**：

对于音乐施加越多的约束，生成的音乐便越不自然。所以引入了超参 **C**，来表示对于音乐的控制程度。C可以由用户来把控。

**节奏点-时间编码**：

节奏点-时间编码在 *train* 与 *inference* 两个阶段均有引入，来告诉CMT模型视频结构。表示了当前节拍数与总节拍数的比率，本文将这个比率划分为M类，然后用同样的嵌入层将其映射到与符号嵌入一样的维度。接下来，把他们加在一起，得到CMT的最终输入。

**音乐类型与乐器种类**：

定义了六种音乐类型（Country, Dance, Electronic, Metal, Pop, and Rock）和五种乐器（Drums，Piano，Guitar，Bass and Strings）。

最终会将上述指标整合在一起。整体算法流程见下图。

![algorithm](/assets/img/1029/algorithm.PNG)

<br>

**【Sequence Modeling】**

以音乐符号序列作为 *linear transformer* 的输入，来建立元素之间的依赖性。

接下来，*multi-head output module* 分两阶段预测每个音乐标记的 7 个属性。第一阶段，模型通过一个线性映射，利用 *transformer* 的输出**预测类型（type）这一属性**。在第二阶段，将类型属性传过 6 个 *feed-forward heads*，以同时预测剩余的 6 个属性值。

在 *inference* 阶段，使用了 *stochastic temperature-controlled sampling* 以增加生成标记的多样性。

<br>

### 实验

<br>

实验进行了客观评估与主观评估。

客观评估主要关注生成音乐的质量，每个音乐种类生成10个乐曲，计算客观指标的平均值。

主观评估通过调研问卷，检验音乐质量与视频的相容度。

<br>

**【数据集】**

**数据集名称**：Lakh Pianoroll Dataset(LPD)，lpd-5-cleansed version.

**数据集规模**：174,154个多音轨钢琴卷，包含五个乐器种类（Drums, Piano, Guitar, Bass, and Strings）。本文选取了 3,038 个MIDI音乐片段作为训练集。这些片段分为六类（Country，Dance，Electronic，Metal，Pop，Rock）。

<br>

**【实验细节】**

根据每个属性字母表的大小，分别决定每个属性的 *embedding size*。这些嵌入后的属性最后会连接成为一个整体。

在模型设置方面，使用了 12 个自注意力层，每一个含 8 个注意力头。其余细节见原文。

![details](/assets/img/1029/details.PNG)

<br>

**【客观评估】**

![objective](/assets/img/1029/objective.PNG)

使用了如下的评价指标：

1. **Pitch Class Histogram Entropy**：评估音乐的声学质量；

2. **Grooving Pattern Similarity**：衡量音乐韵律性；

3. **Structureness Indicator**：衡量音乐的重复结构；

**Overall rank**：三个评价指标的排名结果平均值。

评价的标准并不是这些指标的高低，而是这些指标与真实数据的距离。

通过控制训练变量和baseline之间作比较，得出了以下结论：

**密度和节奏-时间点编码**可以辅助提升音乐的整体结构。

**力度**使得模型可以更容易习得不同音高的组合，以形成同时音，从而得到更好的 *pitch class histogram entropy*。

加入三种条件的限制，可以得到优于baseline的结果。

但是当尝试使用给定的视频控制以上变量时，音乐结构发生了退化。所以使用客观标准评估了**超参 C**，以在结构退化和音乐-视频相容度中取舍得到一个合适的度。

<br>

**【主观评估】**

![subjective](/assets/img/1029/subjective.PNG)

首先陈述了**为什么需要主观评估**。首先是因为主观评估仍是音乐生成模型评价的主流方法；另外，客观的评估指标没有考虑视频和音乐的相容程度。

接下来，介绍了主观评估的**方式和调查规模**。采用了问卷的方式，对36人进行了调查。其中13人有过相关的音乐经验或者对于音乐表演有基本认识，可以被认为是专业人士。**这边对于人员质量的陈述可以借鉴。**

对于音乐的主观评价有如下几个指标：

1. **丰富度（Richness）**：音乐的变化度和趣味性；
2. **正确度（Correctness）**：听感上是否出现一些问题，例如缺音、不协调的和弦或者不合适的乐器使用等。
3. **结构性（Structuredness）**：是否有重复主题等结构特征。

对于视频与音乐的相容度有以下评价指标：

1. **节奏性**：生成的音乐节奏与视频的运动特征有多相合（针对 *speed*）；
2. **对应性**：生成音乐的重音与视频边界或者视觉节奏是否相合。
3. **结构一致性**：视频与音乐的开始、结束时间应该一致，二者均应该有开场、结尾和中间情节等。

调查用户会将视频做出排序，本文将排序结果的平均值作为最终的**Overall rank**。

关于超参数C的选择，最终选择了0.7作为最合适的控制程度。

![C](/assets/img/1029/C.PNG)

下面一段介绍了 **matching score (MS)** 的定义，主要是衡量了从视频和音乐中提取的同时音密度、力度的一致性。最终从排名前五的音乐结果中选取出一个作为最终结果。

结果显示，最终生成的背景音乐整体优于视频配乐，得到的配乐比 baseline 有更好的相容性。虽然生成音乐的旋律性不如真实数据，但是与视频的高度相符性弥补了这一缺点，使得生成的音乐比人工做出的音乐更为适合。（这个话术可以学习一下）

<br>

**【控制准确度】**

衡量了密度、力度和时间等指标的误差。

<br>

**【可视化工作】**

将本文工作与 *baseline* 的 *loss* 曲线做了可视化对比。

<br>

### 总结

<br>

这篇文章的主要探索工作是——视频背景音乐生成。

文章首先建立了视频与音乐间的三种韵律关系。

然后提出了 CMT（Controllable Music Transformer）模型，以达到局部和全局的音乐生成过程控制。

本文提出的方法，不需要成对的音乐-视频数据作为训练数据。

在未来，相关研究可以包括探索更加抽象的视觉——音乐关联，比如情感与类型；使用波形音乐；通过成对数据进行无监督的视听表示学习。

<br>

### 后记

<br>

这是第一篇有关音乐生成论文的笔记，之前的笔记都是关于和弦或者音符嵌入的。所以对于这一篇的内容进行了比较详细的记录。

这篇文章作为ACMM的最佳论文，应该是考虑到了题材的新颖度，这篇文章也多次强调了**生成与视频相符的背景音乐**这一领域的空白性。

本文的工作对于视频信息的提取指标是很合适的，视频的时长、画面变化速度以及转场点等信息确实是视听一致性的关键指标。

但是，音乐指标的选取看起来主要是为了贴合视频。虽然文章多次强调生成的音乐质量较高，但光是凭借一个小节内有多少音符和几个音符同时发声对于似乎不足以生成高质量的音乐作品。画面变化的剧烈程度转化为此时有几个音符同时出现确实是一个比较新颖的想法，但是从乐理层面是否可行尚未可知。未来可以在乐理层面上进一步拓展本文工作。

在这篇文章中遇到的一些问题：

1. *Compound word* 的概念不是很理解。
2. 对于光流法以及 *optical flow field* 和 *optical flow magnitude* 的理解还不是很深入。
3. 4.2的Beat-timing encoding没有仔细看，不是很理解。
4. 什么是 *stochastic temperature-controlled sampling*？
5. 客观评估中的几个评价指标不是很了解。



